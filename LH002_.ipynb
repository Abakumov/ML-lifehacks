{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get started with JupiterLab?\n",
    "\n",
    "**Look at these useful references:**\n",
    "- How to install JupiterLabs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from dnn_app_utils_v3 import *\n",
    "#%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "#plt.rcParams['image.interpolation'] = 'nearest'\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas to Get Familiar With Your Data. \n",
    "\n",
    "The first step in any machine learning project is familiarize yourself with the data. You'll use the Pandas library for this. \n",
    "Pandas is the primary tool data scientists use for exploring and manipulating data. Most people abbreviate pandas in their code as pd. We do this with the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database. Pandas has powerful methods for most things you'll want to do with this type of data.\n",
    "\n",
    "As an example, we'll look at data about home prices in Melbourne, Australia. In the hands-on exercises, you will apply the same processes to a new dataset, which has home prices in Iowa.\n",
    "\n",
    "The example (Melbourne) data is at the file path \n",
    "\n",
    "#../input/melbourne-housing-snapshot/melb_data.csv.\n",
    "\n",
    "We load and explore the data with the following commands:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filepath to variable for easier access\n",
    "# melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\n",
    "# read the data and store data in DataFrame titled melbourne_data\n",
    "# melbourne_data = pd.read_csv(melbourne_file_path) \n",
    "# print a summary of the data in Melbourne data\n",
    "# melbourne_data.describe()\n",
    "# melbourne_data.columns\n",
    "\n",
    "# dropna drops missing values (think of na as \"not available\")\n",
    "# melbourne_data = melbourne_data.dropna(axis=0)\n",
    "# y = melbourne_data.Price\n",
    "# melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "# X = melbourne_data[melbourne_features]\n",
    "# X.describe()\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Data Description\n",
    "\n",
    "The results show 8 numbers for each column in your original dataset. The first number, the count, shows how many rows have non-missing values.\n",
    "\n",
    "Missing values arise for many reasons. For example, the size of the 2nd bedroom wouldn't be collected when surveying a 1 bedroom house. We'll come back to the topic of missing data.\n",
    "\n",
    "The second value is the mean, which is the average. Under that, std is the standard deviation, which measures how numerically spread out the values are.\n",
    "\n",
    "To interpret the min, 25%, 50%, 75% and max values, imagine sorting each column from lowest to highest value. The first (smallest) value is the min. If you go a quarter way through the list, you'll find a number that is bigger than 25% of the values and smaller than 75% of the values. That is the 25% value (pronounced \"25th percentile\"). The 50th and 75th percentiles are defined analogously, and the max is the largest number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn library\n",
    "\n",
    "The steps to building and using a model are:\n",
    "\n",
    "**Define**: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
    "\n",
    "**Fit**: Capture patterns from provided data. This is the heart of modeling.\n",
    "\n",
    "**Predict**: Just what it sounds like\n",
    "\n",
    "**Evaluate**: Determine how accurate the model's predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'melbourne_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c979c118502e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mpredicted_home_prices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmelbourne_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_home_prices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'melbourne_model' is not defined"
     ]
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "# melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Fit model\n",
    "# melbourne_model.fit(X, y)\n",
    "#print(\"Making predictions for the following 5 houses:\")\n",
    "#print(X.head())\n",
    "#print(\"The predictions are\")\n",
    "#print(melbourne_model.predict(X.head()))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_home_prices = melbourne_model.predict(X)\n",
    "mean_absolute_error(y, predicted_home_prices)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "# Define model\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "# Fit model\n",
    "melbourne_model.fit(train_X, train_y)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "val_predictions = melbourne_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
